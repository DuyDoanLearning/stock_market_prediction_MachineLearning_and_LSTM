{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models for stock prediction - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "The data was downloaded from Bloomberg on:\n",
    "* Exchange rate of Vietnam with its major trading partners: the China and the US\n",
    "* Precious metal spot price and future price: Gold, Silver, Palladium, Platinum\n",
    "* Global Stock Indices: Hang Seng Index, Nasdaq 100, Nasdaq Composite, Nikkei 225, SP500, DOJI, Shanghai Shenzhen CSI3000, Shanghai Shenzhen Composite and Singapore Stock Index\n",
    "* Volatility stock index: VIX Index\n",
    "\n",
    "The data will be imported from previous EDA session, which has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Transfer date column to date time\n",
    "import datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'], format = '%m/%d/%Y')\n",
    "\n",
    "# Turn date into index\n",
    "data.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    \"\"\"Generates dataset windows\n",
    "\n",
    "    Args:\n",
    "      series (array of float) - contains the values of the time series\n",
    "      window_size (int) - the number of time steps to include in the feature\n",
    "      batch_size (int) - the batch size\n",
    "      shuffle_buffer(int) - buffer size to use for the shuffle method\n",
    "\n",
    "    Returns:\n",
    "      dataset (TF Dataset) - TF Dataset containing time windows\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "\n",
    "    # Window the data but only take those with the specified size\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the windows by putting its elements in a single batch\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "\n",
    "    # Create tuples with features and labels\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "    # Shuffle the windows\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "    # Create batches of windows\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning model - LSTM\n",
    "\n",
    "We will build two versions of the model\n",
    "- Univariate LSTM prediction (using the target VN-Index price only)\n",
    "- Multivariate LSTM prediction (combine with global stock indices and commodity price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup batch size, epochs\n",
    "epochs = 120\n",
    "window_size = 10\n",
    "batch_size = 64\n",
    "shuffle_buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_time = int(round(len(data) * 0.8))\n",
    "\n",
    "X_train = data_scaled[:train_time]\n",
    "X_test = data_scaled[train_time:]\n",
    "\n",
    "# Define dataset\n",
    "dataset = windowed_dataset(X_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Build model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[1;32m----> 3\u001b[0m     LSTM(\u001b[39m64\u001b[39m,  input_shape \u001b[39m=\u001b[39m (X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m]), return_sequences\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      4\u001b[0m     LSTM(\u001b[39m64\u001b[39m, return_state\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m      5\u001b[0m     Dropout(\u001b[39m0.1\u001b[39m),\n\u001b[0;32m      6\u001b[0m     Dense(\u001b[39m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[39m# Set the learning rate scheduler\u001b[39;00m\n\u001b[0;32m     10\u001b[0m lr_schedule \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mLearningRateScheduler(\n\u001b[0;32m     11\u001b[0m     \u001b[39mlambda\u001b[39;00m epoch: \u001b[39m1e-8\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (epoch \u001b[39m/\u001b[39m \u001b[39m20\u001b[39m)\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential([\n",
    "    LSTM(64,  input_shape = (X_train.shape[1], X_train.shape[2]), return_sequences= True),\n",
    "    LSTM(64, return_state= False),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Set the learning rate scheduler\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10 ** (epoch / 20)\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error')\n",
    "\n",
    "# Check the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "52/52 [==============================] - 7s 54ms/step - loss: 0.0556 - val_loss: 0.1281\n",
      "Epoch 2/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0191 - val_loss: 0.0845\n",
      "Epoch 3/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0175 - val_loss: 0.1090\n",
      "Epoch 4/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0186 - val_loss: 0.0946\n",
      "Epoch 5/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0176 - val_loss: 0.0531\n",
      "Epoch 6/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0166 - val_loss: 0.0681\n",
      "Epoch 7/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0161 - val_loss: 0.0381\n",
      "Epoch 8/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0158 - val_loss: 0.0500\n",
      "Epoch 9/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0150 - val_loss: 0.0370\n",
      "Epoch 10/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 11/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0150 - val_loss: 0.0698\n",
      "Epoch 12/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0145 - val_loss: 0.0527\n",
      "Epoch 13/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0147 - val_loss: 0.0494\n",
      "Epoch 14/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0138 - val_loss: 0.0431\n",
      "Epoch 15/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0145 - val_loss: 0.0804\n",
      "Epoch 16/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0143 - val_loss: 0.0425\n",
      "Epoch 17/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0133 - val_loss: 0.0460\n",
      "Epoch 18/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0154 - val_loss: 0.0391\n",
      "Epoch 19/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0129 - val_loss: 0.0448\n",
      "Epoch 20/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0135 - val_loss: 0.0419\n",
      "Epoch 21/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0137 - val_loss: 0.0537\n",
      "Epoch 22/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0131 - val_loss: 0.0469\n",
      "Epoch 23/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0128 - val_loss: 0.0428\n",
      "Epoch 24/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0127 - val_loss: 0.0301\n",
      "Epoch 25/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0128 - val_loss: 0.0393\n",
      "Epoch 26/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0136 - val_loss: 0.0393\n",
      "Epoch 27/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0121 - val_loss: 0.0412\n",
      "Epoch 28/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0129 - val_loss: 0.0386\n",
      "Epoch 29/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0125 - val_loss: 0.0388\n",
      "Epoch 30/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0123 - val_loss: 0.0384\n",
      "Epoch 31/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0128 - val_loss: 0.0633\n",
      "Epoch 32/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0124 - val_loss: 0.0319\n",
      "Epoch 33/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0123 - val_loss: 0.0287\n",
      "Epoch 34/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0121 - val_loss: 0.0299\n",
      "Epoch 35/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0123 - val_loss: 0.0457\n",
      "Epoch 36/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0127 - val_loss: 0.0567\n",
      "Epoch 37/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0120 - val_loss: 0.0475\n",
      "Epoch 38/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0119 - val_loss: 0.0346\n",
      "Epoch 39/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0124 - val_loss: 0.0384\n",
      "Epoch 40/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0124 - val_loss: 0.0775\n",
      "Epoch 41/120\n",
      "52/52 [==============================] - 2s 43ms/step - loss: 0.0118 - val_loss: 0.0463\n",
      "Epoch 42/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0115 - val_loss: 0.0515\n",
      "Epoch 43/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0116 - val_loss: 0.0734\n",
      "Epoch 44/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0119 - val_loss: 0.0436\n",
      "Epoch 45/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0116 - val_loss: 0.0563\n",
      "Epoch 46/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0115 - val_loss: 0.0380\n",
      "Epoch 47/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0117 - val_loss: 0.0550\n",
      "Epoch 48/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0115 - val_loss: 0.0372\n",
      "Epoch 49/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0112 - val_loss: 0.0606\n",
      "Epoch 50/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0112 - val_loss: 0.0589\n",
      "Epoch 51/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0114 - val_loss: 0.0645\n",
      "Epoch 52/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0112 - val_loss: 0.0470\n",
      "Epoch 53/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0111 - val_loss: 0.0854\n",
      "Epoch 54/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0113 - val_loss: 0.0430\n",
      "Epoch 55/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0108 - val_loss: 0.0329\n",
      "Epoch 56/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0111 - val_loss: 0.0437\n",
      "Epoch 57/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0117 - val_loss: 0.0299\n",
      "Epoch 58/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0110 - val_loss: 0.0359\n",
      "Epoch 59/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0105 - val_loss: 0.0263\n",
      "Epoch 60/120\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.0109 - val_loss: 0.0540\n",
      "Epoch 61/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0109 - val_loss: 0.0488\n",
      "Epoch 62/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0111 - val_loss: 0.0316\n",
      "Epoch 63/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0104 - val_loss: 0.0476\n",
      "Epoch 64/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0106 - val_loss: 0.0433\n",
      "Epoch 65/120\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0106 - val_loss: 0.0385\n",
      "Epoch 66/120\n",
      "52/52 [==============================] - 2s 43ms/step - loss: 0.0111 - val_loss: 0.0466\n",
      "Epoch 67/120\n",
      "52/52 [==============================] - 2s 44ms/step - loss: 0.0107 - val_loss: 0.0441\n",
      "Epoch 68/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0106 - val_loss: 0.0474\n",
      "Epoch 69/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0104 - val_loss: 0.0402\n",
      "Epoch 70/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0107 - val_loss: 0.0476\n",
      "Epoch 71/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0104 - val_loss: 0.0363\n",
      "Epoch 72/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0103 - val_loss: 0.0409\n",
      "Epoch 73/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0105 - val_loss: 0.0330\n",
      "Epoch 74/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0103 - val_loss: 0.0366\n",
      "Epoch 75/120\n",
      "52/52 [==============================] - 2s 43ms/step - loss: 0.0106 - val_loss: 0.0456\n",
      "Epoch 76/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0101 - val_loss: 0.0426\n",
      "Epoch 77/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0108 - val_loss: 0.0572\n",
      "Epoch 78/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0106 - val_loss: 0.0545\n",
      "Epoch 79/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0108 - val_loss: 0.0476\n",
      "Epoch 80/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0104 - val_loss: 0.0354\n",
      "Epoch 81/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0104 - val_loss: 0.0554\n",
      "Epoch 82/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0099 - val_loss: 0.0349\n",
      "Epoch 83/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0106 - val_loss: 0.0287\n",
      "Epoch 84/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0101 - val_loss: 0.0460\n",
      "Epoch 85/120\n",
      "52/52 [==============================] - 2s 44ms/step - loss: 0.0109 - val_loss: 0.0337\n",
      "Epoch 86/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0105 - val_loss: 0.0390\n",
      "Epoch 87/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0105 - val_loss: 0.0583\n",
      "Epoch 88/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0102 - val_loss: 0.0529\n",
      "Epoch 89/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0098 - val_loss: 0.0450\n",
      "Epoch 90/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0101 - val_loss: 0.0520\n",
      "Epoch 91/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0101 - val_loss: 0.0411\n",
      "Epoch 92/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0100 - val_loss: 0.0579\n",
      "Epoch 93/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0102 - val_loss: 0.0509\n",
      "Epoch 94/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0104 - val_loss: 0.0483\n",
      "Epoch 95/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0101 - val_loss: 0.0729\n",
      "Epoch 96/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0098 - val_loss: 0.0584\n",
      "Epoch 97/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0097 - val_loss: 0.0503\n",
      "Epoch 98/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0103 - val_loss: 0.0462\n",
      "Epoch 99/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0099 - val_loss: 0.0440\n",
      "Epoch 100/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0098 - val_loss: 0.0399\n",
      "Epoch 101/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0099 - val_loss: 0.0490\n",
      "Epoch 102/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0103 - val_loss: 0.0439\n",
      "Epoch 103/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0099 - val_loss: 0.0625\n",
      "Epoch 104/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0096 - val_loss: 0.0358\n",
      "Epoch 105/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0098 - val_loss: 0.0449\n",
      "Epoch 106/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0100 - val_loss: 0.0683\n",
      "Epoch 107/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0103 - val_loss: 0.0401\n",
      "Epoch 108/120\n",
      "52/52 [==============================] - 2s 41ms/step - loss: 0.0097 - val_loss: 0.0364\n",
      "Epoch 109/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0100 - val_loss: 0.0549\n",
      "Epoch 110/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0100 - val_loss: 0.0540\n",
      "Epoch 111/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0100 - val_loss: 0.0616\n",
      "Epoch 112/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0102 - val_loss: 0.0463\n",
      "Epoch 113/120\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0098 - val_loss: 0.0435\n",
      "Epoch 114/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0102 - val_loss: 0.0327\n",
      "Epoch 115/120\n",
      "52/52 [==============================] - 2s 45ms/step - loss: 0.0107 - val_loss: 0.0473\n",
      "Epoch 116/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0099 - val_loss: 0.0456\n",
      "Epoch 117/120\n",
      "52/52 [==============================] - 2s 42ms/step - loss: 0.0099 - val_loss: 0.0473\n",
      "Epoch 118/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0095 - val_loss: 0.0504\n",
      "Epoch 119/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0097 - val_loss: 0.0572\n",
      "Epoch 120/120\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0097 - val_loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(trainX, trainy, batch_size = batch_size, epochs = epochs, validation_data= (testX, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/25 [>.............................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# Refit the scaler\n",
    "scaler.fit(data['index_vni'].values.reshape(-1,1))\n",
    "\n",
    "# Inverse scaling\n",
    "predictions_scaled = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
